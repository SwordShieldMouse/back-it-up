@phdthesis{sutton1984,
author = {Sutton, Richard S.},
title = {Temporal Credit Assignment in Reinforcement Learning},
year = {1984},
publisher = {University of Massachusetts Amherst}
}

@inproceedings{todorov2012mujoco,
  author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle = {IROS},
  pages = {5026-5033},
  publisher = {IEEE},
  title = {MuJoCo: A Physics Engine for Model-based Control.},
  year = {2012}
}

@article{schlomerquadpy,
  title={quadpy: Numerical Integration (Quadrature, Cubature) in Python (2018)},
  author={Schl{\"o}mer, Nico},
  journal={URL https://github. com/nschloe/quadpy.[Online}
}

@inproceedings{chou2017improving,
  title={Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution},
  author={Chou, Po-Wei and Maturana, Daniel and Scherer, Sebastian},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={834--843},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{rawlik2013stochastic,
  title={On stochastic optimal control and reinforcement learning by approximate inference},
  author={Rawlik, Konrad and Toussaint, Marc and Vijayakumar, Sethu},
  booktitle={Twenty-third international joint conference on artificial intelligence},
  year={2013}
}

@misc{richter2019learning,
    title={Learning Policies through Quantile Regression},
    author={Oliver Richter and Roger Wattenhofer},
    year={2019},
    eprint={1906.11941},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{levine2018reinforcement,
  title={Reinforcement learning and control as probabilistic inference: Tutorial and review},
  author={Levine, Sergey},
  journal={arXiv preprint arXiv:1805.00909},
  year={2018}
}

@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K},
  booktitle={Aaai},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA}
}

@misc{ward2019improving,
    title={Improving Exploration in Soft-Actor-Critic with Normalizing Flows Policies},
    author={Patrick Nadeem Ward and Ariella Smofsky and Avishek Joey Bose},
    year={2019},
    eprint={1906.02771},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@article{amodei_concrete_2016,
	title = {Concrete {Problems} in {AI} {Safety}},
	url = {https://arxiv.org/abs/1606.06565v2},
	abstract = {Rapid progress in machine learning and artificial intelligence (AI) has
brought increasing attention to the potential impacts of AI technologies on
society. In this paper we discuss one such potential impact: the problem of
accidents in machine learning systems, defined as unintended and harmful
behavior that may emerge from poor design of real-world AI systems. We present
a list of five practical research problems related to accident risk,
categorized according to whether the problem originates from having the wrong
objective function ("avoiding side effects" and "avoiding reward hacking"), an
objective function that is too expensive to evaluate frequently ("scalable
supervision"), or undesirable behavior during the learning process ("safe
exploration" and "distributional shift"). We review previous work in these
areas as well as suggesting research directions with a focus on relevance to
cutting-edge AI systems. Finally, we consider the high-level question of how to
think most productively about the safety of forward-looking applications of AI.},
	language = {en},
	urldate = {2020-06-22},
	author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan},
	month = jun,
	year = {2016},
	keywords = {ai, reward hacking, safety},
	file = {Full Text PDF:C\:\\Users\\Alan_\\Zotero\\storage\\96VN33IH\\Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf:application/pdf;Snapshot:C\:\\Users\\Alan_\\Zotero\\storage\\SQLFEJMR\\1606.html:text/html}
}


@article{tieleman2012lecture,
  title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={COURSERA: Neural networks for machine learning},
  volume={4},
  number={2},
  pages={26--31},
  year={2012}
}

@inproceedings{konda2000actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay R and Tsitsiklis, John N},
  booktitle={Advances in neural information processing systems},
  pages={1008--1014},
  year={2000}
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous Methods for Deep Reinforcement Learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016}
}

@misc{bhandari2019global,
    title={Global Optimality Guarantees For Policy Gradient Methods},
    author={Jalaj Bhandari and Daniel Russo},
    year={2019},
    eprint={1906.01786},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@InProceedings{pardo2017time,
  title = 	 {Time Limits in Reinforcement Learning},
  author = 	 {Pardo, Fabio and Tavakoli, Arash and Levdik, Vitaly and Kormushev, Petar},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4045--4054},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
%   month = 	 {10--15 Jul},
    month = {7},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/pardo18a/pardo18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/pardo18a.html},
  abstract = 	 {In reinforcement learning, it is common to let an agent interact for a fixed amount of time with its environment before resetting it and repeating the process in a series of episodes. The task that the agent has to learn can either be to maximize its performance over (i) that fixed period, or (ii) an indefinite period where time limits are only used during training to diversify experience. In this paper, we provide a formal account for how time limits could effectively be handled in each of the two cases and explain why not doing so can cause state-aliasing and invalidation of experience replay, leading to suboptimal policies and training instability. In case (i), we argue that the terminations due to time limits are in fact part of the environment, and thus a notion of the remaining time should be included as part of the agent’s input to avoid violation of the Markov property. In case (ii), the time limits are not part of the environment and are only used to facilitate learning. We argue that this insight should be incorporated by bootstrapping from the value of the state at the end of each partial episode. For both cases, we illustrate empirically the significance of our considerations in improving the performance and stability of existing reinforcement learning algorithms, showing state-of-the-art results on several control tasks.}
}

@article{gerstner1998numerical,
  title={Numerical Integration using Sparse Grids},
  author={Gerstner, Thomas and Griebel, Michael},
  year={1998},
  journal={Numerical Algorithms},
  volume={18},
  number={3},
  pages={209}
}

@article{ziebart2010modeling,
  title={Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy},
  author={Ziebart, B.},
  year={2010},
  journal={PhD thesis, Carnegie Mellon University}
}

@article{haarnoja2018soft,
  title={Soft Actor-Critic: Off-policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1801.01290},
  year={2018}
}

@article{haarnoja2018sacapplications,
  author    = {Tuomas Haarnoja and
               Aurick Zhou and
               Kristian Hartikainen and
               George Tucker and
               Sehoon Ha and
               Jie Tan and
               Vikash Kumar and
               Henry Zhu and
               Abhishek Gupta and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Soft Actor-Critic Algorithms and Applications},
  journal   = {arXiv preprint arXiv:1812.05905},
  year      = {2018}
}


@book{sutton2018reinforcement,
  title={Reinforcement Learning: An Introduction},
  author={Sutton, Richard S. and Barto, Andrew G.},
  year={2018},
  publisher={MIT press}
}

@inproceedings{wagner2011reinterpretation,
  title={A Reinterpretation of the Policy Oscillation Phenomenon in Approximate Policy Iteration},
  author={Wagner, Paul},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2573--2581},
  year={2011}
}


@inproceedings{perkins2002existence,
author = {Perkins, Theodore J. and Pendrith, Mark D.},
title = {On the Existence of Fixed Points for Q-Learning and Sarsa in Partially Observable Domains},
year = {2002},
isbn = {1558608737},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Nineteenth International Conference on Machine Learning},
pages = {490–497},
numpages = {8},
series = {ICML ’02}
}
  


@article{lim2018actor,
  title={Actor-Expert: A Framework for using Q-learning in Continuous Action Spaces},
  author={Lim, Sungsu and Joseph, Ajin and Le, Lei and Pan, Yangchen and White, Martha},
  journal={arXiv preprint arXiv:1810.09103},
  year={2018}
}

@article{bertsekas2011approximate,
  title={Approximate Policy Iteration: A Survey and Some New Methods},
  author={Bertsekas, Dimitri P.},
  journal={Journal of Control Theory and Applications},
  volume={9},
  number={3},
  pages={310--335},
  year={2011},
  publisher={Springer}
}

@book{bertsekas1996neuro,
  title={Neuro-dynamic programming},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  year={1996},
  publisher={Athena Scientific}
}

@inproceedings{mei2019principled,
  title={On principled entropy exploration in policy optimization},
  author={Mei, Jincheng and Xiao, Chenjun and Huang, Ruitong and Schuurmans, Dale and M{\"u}ller, Martin},
  booktitle={Proceedings of the 28th International Joint Conference on Artificial Intelligence},
  pages={3130--3136},
  year={2019},
  organization={AAAI Press}
}

@book{puterman2014markov,
  title={Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  author={Puterman, Martin L.},
  year={2014},
  publisher={John Wiley \& Sons}
}

@inproceedings{sutton2000policy,
  title={Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  author={Sutton, Richard S. and McAllester, David A. and Singh, Satinder P. and Mansour, Yishay},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1057--1063},
  year={2000}
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@inproceedings{silver2014deterministic,
  title={Deterministic Policy Gradient Algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={Proceedings of the 31 st International Conference on Machine
Learning},
  year={2014}
}

@inproceedings{imani2018off,
  title={An Off-policy Policy Gradient Theorem using Emphatic weightings},
  author={Imani, Ehsan and Graves, Eric and White, Martha},
  booktitle={Advances in Neural Information Processing Systems},
  pages={96--106},
  year={2018}
}

@inproceedings{lazaric2010analysis,
  title={Analysis of a Classification-based Policy Iteration Algorithm},
  author={Lazaric, Alessandro and Ghavamzadeh, Mohammad and Munos, R{\'e}mi},
  year={2010}
}

@InProceedings{thomas2014bias,
  title = 	 {Bias in Natural Actor-Critic Algorithms},
  author = 	 {Philip Thomas},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {441--448},
  year = 	 {2014},
  editor = 	 {Eric P. Xing and Tony Jebara},
  volume = 	 {32},
  number =       {1},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
%   month = 	 {22--24 Jun},
month = {6},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/thomas14.pdf},
  url = 	 {http://proceedings.mlr.press/v32/thomas14.html},
  abstract = 	 {We show that several popular discounted reward natural actor-critics, including the popular NAC-LSTD and eNAC algorithms, do not generate unbiased estimates of the natural policy gradient as claimed. We derive the first unbiased discounted reward natural actor-critics using batch and iterative approaches to gradient estimation. We argue that the bias makes the existing algorithms more appropriate for the average reward setting. We also show that, when Sarsa(lambda) is guaranteed to converge to an optimal policy, the objective function used by natural actor-critics is concave, so policy gradient methods are guaranteed to converge to globally optimal policies as well.}
}

@inproceedings{nota2019policy,
  title={Is the Policy Gradient a Gradient?},
  author={Nota, Chris and Thomas, Philip S.},
%   journal={arXiv preprint arXiv:1906.07073},
%   year={2019}
    booktitle = {Proc. of the 19th International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2020)},
    year = {2020}
}

@article{lillicrap2015continuous,
  title={Continuous Control with Deep Reinforcement Learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
%   journal={arXiv preprint arXiv:1509.02971},
%   year={2015}
journal = {International Conference on Learning Representations},
year = {2016}
}

@article{williams1992simple,
  title={Simple Statistical Gradient-following Algorithms for Connectionist Reinforcement Learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@inproceedings{ryu2019caql,
    title={CAQL: Continuous Action Q-Learning},
    author={Moonkyung Ryu and Yinlam Chow and Ross Anderson and Christian Tjandraatmadja and Craig Boutilier},
    year={2020},
    booktitle={International Conference on Learning Representations}
}

@article{farahmand2015classificationbased,
  title={Classification-based Approximate Policy Iteration},
  author={Farahmand, Amir-massoud and Precup, Doina and Barreto, Andr{\'e} MS and Ghavamzadeh, Mohammad},
  journal={IEEE Transactions on Automatic Control},
  volume={60},
  number={11},
  pages={2989--2993},
  year={2015},
  publisher={IEEE}
}

@inproceedings{scherrer2014local,
  title={Local Policy Search in a Convex Space and Conservative Policy Iteration as Boosted Policy Search},
  author={Scherrer, Bruno and Geist, Matthieu},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={35--50},
  year={2014},
  organization={Springer}
}

@inproceedings{lagoudakis2003reinforcement,
  title={Reinforcement Learning as Classification: Leveraging Modern Classifiers},
  author={Lagoudakis, Michail G and Parr, Ronald},
  booktitle={Proceedings of the 20th International Conference on Machine Learning (ICML-03)},
  pages={424--431},
  year={2003}
}


@InProceedings{schulman2015trust,
  title = 	 {Trust Region Policy Optimization},
  author = 	 {John Schulman and Sergey Levine and Pieter Abbeel and Michael Jordan and Philipp Moritz},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1889--1897},
  year = 	 {2015},
  editor = 	 {Francis Bach and David Blei},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
%   month = 	 {07--09 Jul},
month = {7},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schulman15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/schulman15.html},
  abstract = 	 {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
}

@article{schulman2017proximal,
  title={Proximal Policy Optimization Algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{haarnoja2017reinforcement,
  title={Reinforcement Learning with Deep Energy-based Policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1352--1361},
  year={2017},
  organization={JMLR. org}
}


@article{shani2019adaptive,
  title={Adaptive trust region policy optimization: Global convergence and faster rates for regularized mdps},
  author={Shani, Lior and Efroni, Yonathan and Mannor, Shie},
  journal={arXiv preprint arXiv:1909.02769},
  year={2019}
}

@misc{arjovsky2017wasserstein,
    title={Wasserstein GAN},
    author={Martin Arjovsky and Soumith Chintala and Léon Bottou},
    year={2017},
    eprint={1701.07875},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}


@article{mei_global2020,
	title = {On the {Global} {Convergence} {Rates} of {Softmax} {Policy} {Gradient} {Methods}},
	abstract = {We make three contributions toward better understanding policy gradient methods in the tabular setting. First, we show that with the true gradient, policy gradient with a softmax parametrization converges at a O(1/t) rate, with constants depending on the problem and initialization. This result signiﬁcantly expands the recent asymptotic convergence results. The analysis relies on two ﬁndings: that the softmax policy gradient satisﬁes a Łojasiewicz inequality, and the minimum probability of an optimal action during optimization can be bounded in terms of its initial value. Second, we analyze entropy regularized policy gradient and show that it enjoys a signiﬁcantly faster linear convergence rate O(e−t) toward softmax optimal policy. This result resolves an open question in the recent literature. Finally, combining the above two results and additional new Ω(1/t) lower bound results, we explain how entropy regularization improves policy optimization, even with the true gradient, from the perspective of convergence rate. The separation of rates is further explained using the notion of non-uniform Łojasiewicz degree. These results provide a theoretical understanding of the impact of entropy and corroborate existing empirical studies.},
	year={2020},
	language = {en},
	author = {Mei, Jincheng and Xiao, Chenjun and Szepesvári, Csaba and Schuurmans, Dale},
	pages = {10},
	booktitle={Proceedings of the 37th International Conference on Machine Learning},
}


@misc{islam2019entropy,
    title={Entropy Regularization with Discounted Future State Distribution in Policy Gradient Methods},
    author={Riashat Islam and Raihan Seraj and Pierre-Luc Bacon and Doina Precup},
    year={2019},
    eprint={1912.05104},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{liu2016stein,
    title={Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm},
    author={Qiang Liu and Dilin Wang},
    year={2016},
    eprint={1608.04471},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{liu2017stein,
    title={Stein Variational Policy Gradient},
    author={Yang Liu and Prajit Ramachandran and Qiang Liu and Jian Peng},
    year={2017},
    eprint={1704.02399},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{ilyas2018deep,
    title={A Closer Look at Deep Policy Gradients},
    author={Andrew Ilyas and Logan Engstrom and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},
    year={2020},
    booktitle={International Conference on Learning Representations}
}

@inproceedings{engstrom2019implementation,
  title={Implementation Matters in Deep RL: A Case Study on PPO and TRPO},
  author={Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@book{billingsley2008probability,
  title={Probability and Measure},
  author={Billingsley, Patrick},
  year={2008},
  publisher={John Wiley \& Sons}
}

@article{wang2016sample,
  title={Sample Efficient Actor-Critic with Experience Replay},
  author={Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
  journal={International Conference on Learning Representations},
  year={2017}
}

@article{schulman2015high,
  title={High-dimensional Continuous Control using Generalized Advantage Estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={International Conference on Learning Representations},
  year={2016}
}

@article{nachum2019algaedice,
  title={AlgaeDICE: Policy Gradient from Arbitrary Experience},
  author={Nachum, Ofir and Dai, Bo and Kostrikov, Ilya and Chow, Yinlam and Li, Lihong and Schuurmans, Dale},
  journal={arXiv preprint arXiv:1912.02074},
  year={2019}
}

@inproceedings{nachum2017bridging,
  title={Bridging the Gap between Value and Policy Based Reinforcement Learning},
  author={Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2775--2785},
  year={2017}
}

@article{schulman2017equivalence,
  title={Equivalence between Policy Gradients and Soft Q-learning},
  author={Schulman, John and Chen, Xi and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1704.06440},
  year={2017}
}

@article{petit2019all,
  title={All-Action Policy Gradient Methods: A Numerical Integration Approach},
  author={Petit, Benjamin and Amdahl-Culleton, Loren and Liu, Yao and Smith, Jimmy and Bacon, Pierre-Luc},
  journal={arXiv preprint arXiv:1910.09093},
  year={2019}
}

@article{agarwal2019optimality,
  title={Optimality and Approximation with Policy Gradient Methods in Markov Decision Processes},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={arXiv preprint arXiv:1908.00261},
  year={2019}
}

@article{mahadevan1996average,
  title={Average reward reinforcement learning: Foundations, algorithms, and empirical results},
  author={Mahadevan, Sridhar},
  journal={Machine learning},
  volume={22},
  number={1-3},
  pages={159--195},
  year={1996},
  publisher={Springer}
}

@InProceedings{dadashi2019value,
  title = 	 {The Value Function Polytope in Reinforcement Learning},
  author = 	 {Dadashi, Robert and Taiga, Adrien Ali and Roux, Nicolas Le and Schuurmans, Dale and Bellemare, Marc G.},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1486--1495},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
%   month = 	 {09--15 Jun},
  month = {6},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/dadashi19a/dadashi19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/dadashi19a.html},
  abstract = 	 {We establish geometric and topological properties of the space of value functions in finite state-action Markov decision processes. Our main contribution is the characterization of the nature of its shape: a general polytope (Aigner et al., 2010). To demonstrate this result, we exhibit several properties of the structural relationship between policies and value functions including the line theorem, which shows that the value functions of policies constrained on all but one state describe a line segment. Finally, we use this novel perspective and introduce visualizations to enhance the understanding of the dynamics of reinforcement learning algorithms.}
}


@InProceedings{agarwal2019learning,
  title = 	 {Learning to Generalize from Sparse and Underspecified Rewards},
  author = 	 {Agarwal, Rishabh and Liang, Chen and Schuurmans, Dale and Norouzi, Mohammad},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {130--140},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
%   month = 	 {09--15 Jun},
    month = {6},
  publisher = 	 {PMLR},
%   pdf = 	 {http://proceedings.mlr.press/v97/agarwal19e/agarwal19e.pdf},
%   url = 	 {http://proceedings.mlr.press/v97/agarwal19e.html},
%   abstract = 	 {We consider the problem of learning from sparse and underspecified rewards, where an agent receives a complex input, such as a natural language instruction, and needs to generate a complex response, such as an action sequence, while only receiving binary success-failure feedback. Such success-failure rewards are often underspecified: they do not distinguish between purposeful and accidental success. Generalization from underspecified rewards hinges on discounting spurious trajectories that attain accidental success, while learning from sparse feedback requires effective exploration. We address exploration by using a mode covering direction of KL divergence to collect a diverse set of successful trajectories, followed by a mode seeking KL divergence to train a robust policy. We propose Meta Reward Learning (MeRL) to construct an auxiliary reward function that provides more refined feedback for learning. The parameters of the auxiliary reward function are optimized with respect to the validation performance of a trained policy. The MeRL approach outperforms an alternative method for reward learning based on Bayesian Optimization, and achieves the state-of-the-art on weakly-supervised semantic parsing. It improves previous work by 1.2% and 2.4% on WikiTableQuestions and WikiSQL datasets respectively.}
}

@inproceedings{duan2016benchmarking,
  title={Benchmarking Deep Reinforcement Learning for Continuous Control},
  author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={1329--1338},
  year={2016}
}

@INPROCEEDINGS{seijen2009, author={H. {van Seijen} and H. {van Hasselt} and S. {Whiteson} and M. {Wiering}}, booktitle={2009 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning}, title={A Theoretical and Empirical Analysis of Expected Sarsa}, year={2009}, volume={}, number={}, pages={177-184},}

@inproceedings{
abdolmaleki2018maximum,
title={Maximum a Posteriori Policy Optimisation},
author={Abbas Abdolmaleki and Jost Tobias Springenberg and Yuval Tassa and Remi Munos and Nicolas Heess and Martin Riedmiller},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=S1ANxQW0b},
}

@inproceedings{fellows2019virel,
  title={Virel: A Variational Inference Framework for Reinforcement Learning},
  author={Fellows, Matthew and Mahajan, Anuj and Rudner, Tim GJ and Whiteson, Shimon},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7120--7134},
  year={2019}
}

@inproceedings{neumann2011variational,
  title={Variational inference for policy search in changing situations},
  author={Neumann, Gerhard and others},
  booktitle={Proceedings of the 28th International Conference on Machine Learning, ICML 2011},
  pages={817--824},
  year={2011}
}

@inproceedings{kober2009policy,
  title={Policy search for motor primitives in robotics},
  author={Kober, Jens and Peters, Jan R},
  booktitle={Advances in neural information processing systems},
  pages={849--856},
  year={2009}
}


@InProceedings{geist2019theory,
  title = 	 {A Theory of Regularized {M}arkov Decision Processes},
  author = 	 {Geist, Matthieu and Scherrer, Bruno and Pietquin, Olivier},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2160--2169},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
%   month = 	 {09--15 Jun},
month = {6},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/geist19a/geist19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/geist19a.html},
  abstract = 	 {Many recent successful (deep) reinforcement learning algorithms make use of regularization, generally based on entropy or Kullback-Leibler divergence. We propose a general theory of regularized Markov Decision Processes that generalizes these approaches in two directions: we consider a larger class of regularizers, and we consider the general modified policy iteration approach, encompassing both policy iteration and value iteration. The core building blocks of this theory are a notion of regularized Bellman operator and the Legendre-Fenchel transform, a classical tool of convex optimization. This approach allows for error propagation analyses of general algorithmic schemes of which (possibly variants of) classical algorithms such as Trust Region Policy Optimization, Soft Q-learning, Stochastic Actor Critic or Dynamic Policy Programming are special cases. This also draws connections to proximal convex optimization, especially to Mirror Descent.}
}

@misc{vieillard2020leverage,
    title={Leverage the Average: an Analysis of Regularization in RL},
    author={Nino Vieillard and Tadashi Kozuno and Bruno Scherrer and Olivier Pietquin and Rémi Munos and Matthieu Geist},
    year={2020},
    eprint={2003.14089},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{young2019minatar,
  title={MinAtar: An Atari-inspired Testbed for More Efficient Reinforcement Learning Experiments},
  author={Young, Kenny and Tian, Tian},
  journal={arXiv preprint arXiv:1903.03176},
  year={2019}
}

@InProceedings{ahmed2018understanding,
  title = 	 {Understanding the Impact of Entropy on Policy Optimization},
  author = 	 {Ahmed, Zafarali and Le Roux, Nicolas and Norouzi, Mohammad and Schuurmans, Dale},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {151--160},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
%   month = 	 {09--15 Jun},
month = {6},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/ahmed19a/ahmed19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/ahmed19a.html},
  abstract = 	 {Entropy regularization is commonly used to improve policy optimization in reinforcement learning. It is believed to help with exploration by encouraging the selection of more stochastic policies. In this work, we analyze this claim using new visualizations of the optimization landscape based on randomly perturbing the loss function. We first show that even with access to the exact gradient, policy optimization is difficult due to the geometry of the objective function. We then qualitatively show that in some environments, a policy with higher entropy can make the optimization landscape smoother, thereby connecting local optima and enabling the use of larger learning rates. This paper presents new tools for understanding the optimization landscape, shows that policy entropy serves as a regularizer, and highlights the challenge of designing general-purpose policy optimization algorithms.}
}

@article{neu2017unified,
  title={A Unified View of Entropy-regularized Markov Decision Processes},
  author={Neu, Gergely and Jonsson, Anders and G{\'o}mez, Vicen{\c{c}}},
  journal={arXiv preprint arXiv:1705.07798},
  year={2017}
}

@inproceedings{liu2019neural,
  title={Neural Trust Region/Proximal Policy Optimization Attains Globally Optimal Policy},
  author={Liu, Boyi and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10564--10575},
  year={2019}
}

@inproceedings{auer1996exponentially,
  title={Exponentially Many Local Minima For Single Neurons},
  author={Auer, Peter and Herbster, Mark and Warmuth, Manfred KK},
  booktitle={Advances in neural information processing systems},
  pages={316--322},
  year={1996}
}

@inproceedings{chen2019surrogate,
  title={Surrogate Objectives for Batch Policy Optimization in One-step Decision Making},
  author={Chen, Minmin and Gummadi, Ramki and Harris, Chris and Schuurmans, Dale},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8825--8835},
  year={2019}
}


@article{kingma2014adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={International Conference on Learning Representations},
  year={2015}
}

@article{clenshaw1960method,
  title={A Method for Numerical Integration on an Automatic Computer},
  author={Clenshaw, Charles W and Curtis, Alan R},
  journal={Numerische Mathematik},
  volume={2},
  number={1},
  pages={197--205},
  year={1960},
  publisher={Springer}
}

@incollection{norouzi2016reward,
title = {Reward Augmented Maximum Likelihood for Neural Structured Prediction},
author = {Norouzi, Mohammad and Bengio, Samy and Chen, zhifeng and Jaitly, Navdeep and Schuster, Mike and Wu, Yonghui and Schuurmans, Dale},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {1723--1731},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6547-reward-augmented-maximum-likelihood-for-neural-structured-prediction.pdf}
}


@inproceedings{nachum2016improving,
    title={Improving Policy Gradient by Exploring Under-appreciated Rewards},
    author={Ofir Nachum and Mohammad Norouzi and Dale Schuurmans},
    booktitle = {International Conference on Learning Representations},
    year={2017}
}

%% Policy-iteration stuff
@inproceedings{kakade2002approximately,
  title={Approximately Optimal Approximate Reinforcement Learning},
  author={Kakade, Sham and Langford, John},
  booktitle={ICML},
  volume={2},
  pages={267--274},
  year={2002}
}

@inproceedings{kakade2002natural,
  title={A Natural Policy Gradient},
  author={Kakade, Sham M.},
  booktitle={Advances in neural information processing systems},
  pages={1531--1538},
  year={2002}
}

@inproceedings{perkins2003convergent,
  title={A Convergent Form of Approximate Policy Iteration},
  author={Perkins, Theodore J and Precup, Doina},
  booktitle={Advances in neural information processing systems},
  pages={1627--1634},
  year={2003}
}

@article{brockman2016openai,
  title={OpenAI Gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{walt2011numpy,
  title={The NumPy Array: a Structure for Efficient Numerical Computation},
  author={Walt, St{\'e}fan van der and Colbert, S Chris and Varoquaux, Gael},
  journal={Computing in Science \& Engineering},
  volume={13},
  number={2},
  pages={22--30},
  year={2011},
  publisher={IEEE Computer Society}
}

@inproceedings{paszke2019pytorch,
  title={PyTorch: An Imperative Style, High-performance Deep Learning Library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8024--8035},
  year={2019}
}

@article{kingma2013auto,
  title={Auto-encoding Variational Bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={International Conference on Learning Representations},
  year={2014}
}

@inproceedings{hasselt2010double,
  title={Double Q-learning},
  author={Hasselt, Hado V},
  booktitle={Advances in neural information processing systems},
  pages={2613--2621},
  year={2010}
}

@article{mnih2015human,
  title={Human-level Control through Deep Reinforcement Learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{wagner2013optimistic,
  title={Optimistic Policy Iteration and Natural Actor-Critic: A Unifying View and a Non-optimality Result},
  author={Wagner, Paul},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1592--1600},
  year={2013}
}

@article{o2016combining,
  title={Combining Policy gradient and Q-learning},
  author={O'Donoghue, Brendan and Munos, Remi and Kavukcuoglu, Koray and Mnih, Volodymyr},
  journal={International Conference on Learning Representations},
  year={2017}
}

@inproceedings{vieillard2019deep,
  title={Deep Conservative Policy Iteration},
  author={Vieillard, Nino and Pietquin, Olivier and Geist, Matthieu},
  booktitle={Proceedings of the
34th AAAI Conference on Artificial Intelligence.},
  year={2020}
}

@inproceedings{scherrer2014approximate,
  title={Approximate Policy Iteration Schemes: A Comparison},
  author={Scherrer, Bruno},
  booktitle={International Conference on Machine Learning},
  pages={1314--1322},
  year={2014}
}