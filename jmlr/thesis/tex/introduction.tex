% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\chapter{Introduction}

% Here is a test reference~\cite{Knuth68:art_of_programming}.
% These additional lines have been added just to demonstrate the spacing
% for the rest of the document. Spacing will differ between the typeset main
% document, and typeset individual documents, as the commands
% to change spacing for the body of the thesis are only in the main document.

% \section{Cross-Referencing}\label{sec:crossRef}

% Cross-references between child documents are possible using the
% \href{https://www.ctan.org/pkg/zref}{\texttt{zref}} package.

% \newpage

% Text on a new page, to test top margin size.

% A sample equation \eqref{eq:test} follows:

% \begin{equation}
% y = \frac{1}{x^2} + 4 \label{eq:test}
% \end{equation}

% A sample table, Table \ref{tab:test}:

% \begin{table}[h]
%     \centering
%     \begin{tabulary}{0.75\textwidth}{r|L}
%     \textbf{Non-wrapping column} & \textbf{Wrapping column} \\ \hline
%     This is an ordinary column & This is a balanced-width column, where text will wrap
%     \end{tabulary}
%     \caption[A sample table] {A sample table created using the \href{https://ctan.org/pkg/tabulary}{\texttt{tabulary}} package}
%     \label{tab:test}
% \end{table}

% If there are many acronyms, such as \gls{asa}, and specialized technical terms, consider adding a glossary.
% Sample \gls{sampleGlossaryEntry}, and acroynm (\gls{asa}) descriptions are provided above.

A central issue in artificial intelligence (AI) research is the development of agents that can learn or improve themselves based upon interaction with the world. Just as humans can learn without explicit supervision, we desire artificial agents that can discover knowledge about the world through independent exploration and inquiry. After all, if we want AI systems to help us solve problems that we might not be able to solve, how much help would those agents be if they needed explicit human supervision all the time?

% \section{Reinforcement Learning}

Much recent attention has focused on the design of agents that can learn without explicit supervision, perhaps best typified by the field of \textit{reinforcement learning} (RL). In reinforcement learning, researchers try to develop agents that can learn to improve their own performance over time, according to a quantity called a \textit{reward function} that is (usually) specified.%\footnote{Classically, the reward function is specified by the designer of the system, but recent work has attempted to learn a reward function when such a specification is difficult or impossible. See also issues to do with reward hacking \citep{amodei_concrete_2016}.} 
The reward function encapsulates a desired \textit{goal} of the designer: for example, a reward function may return 1 if an agent reaches the end of an obstacle course, and return 0 otherwise. Maximizing the rewards received is taken to be a proxy for achieving the goal. Such a view is not uncontroversial, but is useful in permitting the application of mathematical tools to the RL problem. 

A RL agent observes the world, taking actions that subsequently change the world and (hopefully) bring about the desired goal by maximizing reward. A compact way of summarizing what actions an RL agent might take is known as a \textit{policy}, which provides possible actions for each possible state of the world. A policy may itself be represented by a collection of \textit{parameters} (i.e., some numbers, like the parameters of a neural network), which collectively determine what actions an agent may take in each state. The aim of RL is therefore to develop agents that improve their own policies so as to achieve goals. 

% \section{Policy Optimization}

In service of this aim, one area of inquiry in reinforcement learning is \textit{policy optimization}, where analogies are drawn between policy improvement and the mathematical field of optimization. In optimization, one is typically interested in maximizing (or minimizing) a certain quantity called an \textit{objective function}. Much work in optimization can trace its roots to the 20th century,\footnote{Even earlier origins exist as well, as typified in the works of Fermat, Lagrange, Newton, Cauchy, and Gauss.} where the demands of war and command economies necessitated the development of mathematical techniques that told you how to get the best ``bang for the buck'', so to speak. The fit between RL and optimization seems natural given the proxy definition of goals as rewards. 


More technically, policy optimization involves explicit, parameterized policies that are modified to maximize an objective function, commonly taken to be a sum of rewards. A policy that maximizes the objective function is known as an \textit{optimal policy}. The most popular methods are policy gradient (PG) methods, which iteratively update the policy parameters using the gradient of either the discounted return or the average reward, given by the {policy gradient theorem} \citep{sutton2000policy}.
Although not a recent invention, with early work introducing actor-critic \citep{sutton1984,konda2000actor}, PG methods have recently seen a surge of renewed interest given their ease of application in high-dimensional, continuous action spaces when combined with neural networks \citep{schulman2015high,wang2016sample,mnih2016asynchronous}. Recent developments include learning deterministic polices \citep{silver2014deterministic,lillicrap2015continuous}, trust-regions \citep{schulman2015trust,schulman2017proximal}, continuous-action extensions of Q-learning \citep{ haarnoja2017reinforcement,lim2018actor,ryu2019caql}, probabilistic approaches \citep{abdolmaleki2018maximum,fellows2019virel}, and entropy regularization \citep{ziebart2008maximum, ziebart2010modeling, rawlik2013stochastic, haarnoja2017reinforcement, haarnoja2018soft, levine2018reinforcement}.

Theoretical and empirical work into PG is growing. Theoretically, CPI \citep{kakade2002approximately} is an early example of theoretical insights into obtaining guaranteed policy improvement with PG . More recently, \citet{agarwal2019optimality} derive finite-sample and approximation bounds for a variety of PG methods; \citet{mei_global2020} show that, with a softmax policy parameterization, entropy-regularized PG converges faster than unregularized PG; \citet{neu2017unified,liu2019neural,shani2019adaptive} reformulate TRPO as mirror descent and prove convergence; \citet{ahmed2018understanding} show that entropy regularization may lead to smoother optimization landscapes; and \citet{bhandari2019global}\footnote{We note that the results in \citet{bhandari2019global} bear a striking resemblance to earlier results in \citet{scherrer2014local}. In particular, both works rely on convex policy classes and a closure of the policy class under greedification.} show global optimality of local minima under certain restrictions on the MDP.

Empirically, recent investigations have both unveiled some of the shortcomings and increased our understanding of current PG methods. Many implementations of PG in practice use an incorrect, simplified gradient \citep{thomas2014bias,imani2018off,nota2019policy}. Somewhat unfortunately, the apparent superiority of some more modern PG methods seem to be due to code-level optimizations rather than algorithmic advances \citep{ilyas2018deep,engstrom2019implementation}. In the context of entropy-regularized PG, \citep{ahmed2018understanding} suggest that entropy-regularized PG methods have smoother optimization landscapes than non-regularized methods. 

One way to provide clarity in the analysis of PG methods is to revisit the insight that many PG methods can be seen as approximate policy iteration (API). To find an optimal policy, API methods \citep{bertsekas2011approximate,scherrer2014approximate} interleave approximate policy evaluation---understanding how a policy is currently performing with a value function---and (approximate) policy improvement--making the current policy better based on policy-evaluation information. The policy improvement step is sometimes called the \textit{greedification} step, which refers to the fact that in exact PI, the subsequent policy is set to be the greedy action at each state (i.e., the action that maximizes the current action-value function). 

The connection between PG and \emph{Approximate} PI arises because the efficient implementation of PG methods often requires the estimation of a value function. One could estimate the PG purely through Monte Carlo samples, just as REINFORCE \citep{williams1992simple} does, but it is less wasteful to use data to inform future estimates of the gradient. In particular, with environment data, one is able to estimate the action-value function with temporal-difference methods \citep{sutton2018reinforcement}. Numerous papers have linked PG methods to policy iteration \citep{sutton2000policy,kakade2002approximately,perkins2002existence,perkins2003convergent,wagner2011reinterpretation,wagner2013optimistic,scherrer2014local,bhandari2019global}, including recent work connecting maximum-entropy PG and value-based methods \citep{o2016combining,nachum2017bridging, schulman2017equivalence, nachum2019algaedice}.

Viewing one gradient step (PG) as a policy greedification step (API) suggests that one way to understand PG methods is to understand their greedification steps; in particular, after greedifying, what is the quality of the resulting policy? For tabular policies, policy greedification is straightforward: at each state, we set the policy to place unit mass on the greedy action (or mass spread arbitrarily around the greedy actions), with zero mass on non-greedy actions. If a new policy is greedy with respect to the action-value function of an old policy, the classical policy improvement theorem \citep{sutton2018reinforcement} guarantees that the new policy is at least as good as the old policy. For parameterized policies (e.g., neural-network policies), however, exact greedification in ecah state is rarely possible as not all policies will be representable by a given function approximator class. 

To define an approximate greedification scheme, one can minimize the KL divergence of the current policy to a Boltzmann distribution over the action values \citep{wagner2011reinterpretation}. The use of a Boltzmann distribution is common in pseudo-likelihood methods \citep{kober2009policy,neumann2011variational,levine2018reinforcement}, ensuring that one has a target distribution based on the action-values. The KL divergence is a convenient choice because stochastically estimating the objective only requires the ability to sample from the distributions and evaluate the values of the distributions at single points. It is unclear, however, whether to use the reverse or the forward KL divergence. That is, should the policy $\pi$ be the first argument of the KL divergence, or should it be the Boltzmann distribution over the action values? For example, \citet{neumann2011variational} argues in favour of the reverse KL divergence as such a resulting policy would be cost-averse, while \citet{norouzi2016reward} uses the forward KL divergence to induce a policy that is more exploratory (i.e., has a more diverse state visitation distribution). 

The policy update for many PG methods can be seen as optimizing a reverse KL, though some work has employed the forward KL \citep{norouzi2016reward,nachum2016improving,agarwal2019learning,vieillard2019deep}, including implicitly some of the work in classification for RL \citep{lagoudakis2003reinforcement,lazaric2010analysis,farahmand2015classificationbased}. Despite the fact that both have been used, there is no comprehensive investigation into the differences between these two choices for approximate greedification; the typical default is the reverse KL. The reverse KL without entropy regularization corresponds to a standard actor-critic update and is easy to compute. More recently, it was shown that the reverse KL guarantees policy improvement when the KL can be minimized separately for each state \citep[p.~4]{haarnoja2018soft}. At the same time, reverse KL objectives have known problems, primarily that they are non-convex, even in an ideal case with a linear Boltzmann policy. For contextual bandits, \citet{chen2019surrogate} showed improved performance when using a surrogate, forward KL objective for the smoothed risk. Some works also use the forward KL ostensibly to prevent mode collapse, given that the forward KL is mode-covering \citep{agarwal2019learning,mei2019principled}. The forward KL divergence is also used in supervised learning, in the form of the cross-entropy loss. 
 
No work thus far has explored the difference between the KLs in the framework of what policy improvement guarantees can be provided, and whether the conditions of any such policy improvement results hold in experimental settings. 
 
 
% \section{This Work}
\textbf{The goal of this thesis is to investigate how one should perform the greedification step for parameterized policies.} In particular, for a given action-value estimate, we investigate the difference between using a forward or reverse KL divergence, primarily in the context of entropy regularisation. We ask, given that we optimize a policy to reduce either the forward or the reverse KL divergence to a Boltzmann distribution over the action values, what is the quality of the resulting policy?


We provide some clarity on this question with the following contributions. 
\begin{enumerate}
    \item We highlight four choices for greedication: forward or reverse KL to a Boltzmann distribution on the action-values, with or without entropy regularization.
    
    \item We show that the policy improvement result for the reverse KL extends to certain function-approximation settings.
    
    \item We construct a counterexample where optimizing for the forward KL can fail to induce policy improvement. 
    
    \item Nevertheless, we show that under some additional conditions on the temperature, KL reduction, and entropy, optimizing for the forward KL can induce policy improvement.
    
    \item On small-scale experiments, we find that the reverse KL can converge faster, but sometimes to worse solutions, than the forward KL, particularly under continuous actions. However, depending on the degree of entropy regularization, the forward KL can provide worse solutions than the reverse KL. 
    
    \item On large-scale benchmarks, we found no consistent superiority of either KL divergence over the other, but we note intriguing trends influenced by the forward KL, entropy regularization, and the function approximation architecture.
\end{enumerate}
 



\end{document}